{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from dataloader.create_bias_dataset import CreateGenderStsb\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "T5_PATH = 't5-large' # change the size of the model here. \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "model = T5ForConditionalGeneration.from_pretrained(T5_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results derived from the experiments included in the 'run_bias_experiments.py' file, indicate that we should now\n",
    "focus on these selected professions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_careers = ['nurse', 'engineer', 'surgeon', 'scientist', 'receptionist', 'programmer', 'teacher', 'officer', 'homemaker']\n",
    "#also added 'homemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a df that contains identical sentences for each one of the aforementioned occupations and the words: 'she'\n",
    "and 'he'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(occupation, data_dir=None, lang=None):\n",
    "    dataset_creator = CreateGenderStsb(lang=lang, data_dir=data_dir, occupation=occupation, multilingual=False)\n",
    "    women_df, men_df = dataset_creator.create_gendered_dataframes()\n",
    "    df = pd.concat([\n",
    "        women_df[['sentence1']],\n",
    "        men_df[['sentence1', 'occupation']]\n",
    "    ], axis=1)\n",
    "    df.columns = ['she', 'he', f'{occupation}']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n",
      "len of test set: (1379, 7)\n"
     ]
    }
   ],
   "source": [
    "df1 = create_dataset('technician', data_dir='./data/stsbenchmark', lang='en')\n",
    "for occupation in selected_careers:\n",
    "    df = create_dataset(occupation, data_dir='./data/stsbenchmark', lang='en')\n",
    "    df_all_occs = pd.concat([df1, df[[f'{occupation}']]], axis=1)\n",
    "    df1 = df_all_occs\n",
    "df = df1\n",
    "df.to_csv('./selected_occs.csv', index=False)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(df.duplicated()==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "define functions to extract the embeddings from T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_tokens_and_embeds(sentence):\n",
    "  \"\"\"\n",
    "  returns a tuple of size=len(tokens) populated by (token,embed)'s\n",
    "  \"\"\"\n",
    "  input = str(sentence)\n",
    "  labels = tokenizer('',return_tensors='pt').input_ids.to(DEVICE)\n",
    "  ids = tokenizer(input, return_tensors='pt').input_ids.to(DEVICE)\n",
    "\n",
    "  out = model(input_ids=ids,labels = labels, output_hidden_states=True)\n",
    "  tokens = tokenizer.tokenize(input)\n",
    "  #early_embeds = out.encoder_hidden_states[0]\n",
    "  #early_embeds = early_embeds[:-1]\n",
    "  embeds = out.encoder_last_hidden_state.squeeze(dim=0)\n",
    "  embeds = embeds[:-1] # discard </s> embdedding. \n",
    "  tok_embeds = tuple(zip(tokens,embeds))\n",
    "  return tok_embeds\n",
    "\n",
    "def get_embed(tok_embeds, token):\n",
    "  \"\"\"\n",
    "  given a single specific token, it returns its embedding\n",
    "  exists only to be used in the get_occupation_embed method \n",
    "  \"\"\"\n",
    "  for i in range(len(tok_embeds)):\n",
    "    tok = tok_embeds[i][0]\n",
    "    embed = tok_embeds[i][1]\n",
    "    if tok == token:\n",
    "      # returns word's token alongside with corresponding embedding\n",
    "      return tok, embed.cpu().detach().numpy()\n",
    "    \n",
    "def get_occupation_embed(tok_embeds, word):\n",
    "  \"\"\"\n",
    "  given a word, it returns its embedding or \n",
    "  if the word has to be tokenized into more the one embeddings by the tokenizer, \n",
    "  it returns the sum/mean of those embeddings \n",
    "  (went with mean because this is better for measuring the angles later on,\n",
    "  sum would change the result to a very big vector)\n",
    "  \"\"\"\n",
    "  tokens = tokenizer.tokenize(word)\n",
    "  if len(tokens)==1:\n",
    "    return get_embed(tok_embeds, tokens[0])\n",
    "  else:\n",
    "    whole_word = ''\n",
    "    embed_sum = np.zeros(model.config.hidden_size)\n",
    "    for i in tok_embeds:\n",
    "      if i[0] in tokens:\n",
    "        whole_word += i[0]\n",
    "        embed_sum += i[1].cpu().detach().numpy() # add rest of tokens if any \n",
    "    return (whole_word, embed_sum/len(tokens))\n",
    "\n",
    "def dotproduct(v1, v2):\n",
    "  return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "def length(v):\n",
    "  return math.sqrt(dotproduct(v, v))\n",
    "  \n",
    "def angle(v1, v2):\n",
    "  return (dotproduct(v1, v2) / (length(v1) * length(v2))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Populate the dataframe with the contextualized embeddings of 'he','she' and the occupations' embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['she_embed'] = df['she'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'She'))\n",
    "df['he_embed'] = df['he'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'He'))\n",
    "df['nurse_embed'] = df['nurse'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'nurse'))\n",
    "df['engineer_embed'] = df['engineer'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'engineer'))\n",
    "df['surgeon_embed'] = df['surgeon'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'surgeon'))\n",
    "df['scientist_embed'] = df['scientist'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'scientist'))\n",
    "df['receptionist_embed'] = df['receptionist'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'receptionist'))\n",
    "df['programmer_embed'] = df['programmer'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'programmer'))\n",
    "df['teacher_embed'] = df['teacher'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'teacher'))\n",
    "df['homemaker_embed'] = df['homemaker'].apply(lambda x: get_occupation_embed(get_sentence_tokens_and_embeds(x), 'homemaker'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Collect the embeddings as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hes = [df.loc[i,'he_embed'][1] for i in range(len(df))]\n",
    "shes = [df.loc[i,'she_embed'][1] for i in range(len(df))]\n",
    "nurses = [df.loc[i,'nurse_embed'][1] for i in range(len(df))]\n",
    "engineers = [df.loc[i,'engineer_embed'][1] for i in range(len(df))]\n",
    "surgeons = [df.loc[i,'surgeon_embed'][1] for i in range(len(df))]\n",
    "scientists = [df.loc[i,'scientist_embed'][1] for i in range(len(df))]\n",
    "receptionists = [df.loc[i,'receptionist_embed'][1] for i in range(len(df))]\n",
    "programmers = [df.loc[i,'programmer_embed'][1] for i in range(len(df))]\n",
    "teachers = [df.loc[i,'teacher_embed'][1] for i in range(len(df))]\n",
    "homemakers = [df.loc[i,'homemaker_embed'][1] for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We explore the variation of the she-he distances and angles, to see whether a somewhat stable gender direction\n",
    "exists in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g_distances=[]\n",
    "g_angles=[]\n",
    "for i in range(len(hes)):\n",
    "  d_i = shes[i]-hes[i]\n",
    "  a_i = angle(shes[i],hes[i])\n",
    "  g_distances.append(np.linalg.norm(d_i))\n",
    "  g_angles.append(a_i)\n",
    "\n",
    "mean_distance = np.mean(g_distances)\n",
    "mean_angle = np.mean(g_angles)\n",
    "print(f'she-he distance has a mean of {mean_distance} ± std {np.std(g_distances)}')\n",
    "print(f'she-he angle has a mean of {mean_angle} ± std {np.std(g_angles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also take a look at the variation of those angle-values on a polar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r = np.ones(len(g_angles))\n",
    "theta = np.array(g_angles)\n",
    "area = 40 * r**2\n",
    "colors = theta\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(projection='polar')\n",
    "ax.set_thetamin(0)\n",
    "ax.set_thetamax(90)\n",
    "ax.tick_params(axis='both', which='major', labelsize=25)\n",
    "c = ax.scatter(theta, r, s=area, alpha=0.15, color='r') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be using a mean g vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = np.zeros(model.config.hidden_size)\n",
    "for i in range(len(hes)):\n",
    "  g_i = hes[i] - shes[i]\n",
    "  g += g_i\n",
    "g = g/model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define function to calculate b_i:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def b_i(item_i, g):\n",
    "  angles = []\n",
    "  ang = 0\n",
    "  for i in range(len(item_i)): \n",
    "    a = angle(item_i[i], g)\n",
    "    ang += a\n",
    "    mean_ang = (ang/len(item_i))\n",
    "    angles.append(a)\n",
    "  return angles, math.degrees(mean_ang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot bi distributions for 'she' and 'he':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(b_i(hes,g)[0], bins=50, label='he', density=True, stacked=True, color='Blue')\n",
    "plt.hist(b_i(shes,g)[0], bins=50, label='she', density=True, stacked=True, color='Grey')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Include the occupations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
    "\n",
    "plt.hist(b_i(hes,g)[0], bins=60, label='he', density=True, stacked=True, color='#377eb8')\n",
    "plt.hist(b_i(shes,g)[0], bins=60, label='she', density=True, stacked=True, color='#ff7f00')\n",
    "plt.hist(b_i(nurses,g)[0], bins=60, label='nurse', density=True, stacked=True, color='#4daf4a', alpha=0.7)\n",
    "plt.hist(b_i(engineers,g)[0], bins=60, label='engineer', density=True, stacked=True, color='#f781bf',alpha=0.7)\n",
    "plt.hist(b_i(surgeons,g)[0], bins=60, label='surgeon', density=True, stacked=True, color='#a65628',alpha=0.7)\n",
    "plt.hist(b_i(scientists,g)[0], bins=60, label='scientist', density=True, stacked=True, color='#984ea3',alpha=0.7)\n",
    "plt.hist(b_i(receptionists,g)[0], bins=60, label='receptionist', density=True, stacked=True, color='#999999',alpha=0.7)\n",
    "plt.hist(b_i(programmers,g)[0], bins=60, label='programmer', density=True, stacked=True, color='#e41a1c',alpha=0.7)\n",
    "plt.hist(b_i(teachers,g)[0], bins=60, label='teacher', density=True, stacked=True, color='#dede00',alpha=0.7)\n",
    "plt.hist(b_i(homemakers,g)[0], bins=60, label='homemaker', density=True, stacked=True, color='black',alpha=0.7)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title(T5_PATH)\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.axvline(np.median(b_i(hes,g)[0]), linestyle='dashed', linewidth=2)\n",
    "plt.axvline(np.median(b_i(shes,g)[0]), linestyle='dashed', linewidth=2)\n",
    "plt.xlim(left=-0.5) \n",
    "plt.xlim(right=0.5) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
